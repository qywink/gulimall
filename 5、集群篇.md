---
typora-copy-images-to: assets
typora-root-url: assets
---



![1599806025370](/1599806025370.png)









https://www.kubernetes.org.cn/7315.html：根据这片文章的脉络开始结合本md学习

# 一、K8S

## 1、简介

```
Kubernetes.简称k8s。是用于自动部署，扩展和管理容器化应用程序的开源系统。
【可以分布式的部署整个系统，管理整个系统】
中文官网:https://kubernetes.io/zh/【推荐这个，然后点击 （学习 Kubernetes 基础知识）】
中文社区:https://www.kubernetes.org.cn/
官方文档:https://kubernetes.io/zh/docs/home/
社区文档:http://docs.kubernetes.org.cn/
```

### 1）与传统部署方式对比

https://kubernetes.io/zh/docs/concepts/overview/what-is-kubernetes/ 

```
1、传统方式：硬件+OS+应用程序环境【应用打包】
	弊端：
		1）资源分配问题【多个应用之间资源不隔离，没有明确的界限（一个服务宕机可能导致所有宕机）】
		2）迁移问题，每个应用的环境都需要迁移
		
2、虚拟化部署时代：在OS基础上运行多个虚拟机
	优点：
		1）资源隔离
		2）降低硬件成本
	弊端：
    	1）资源浪费：每一个虚拟机都有一套完整的系统

3、容器部署时代：
	优点：
		1）资源节省
		2）迁移方便：容器打包成镜像运行在任何一个安装了docker的服务器上
		3）资源隔离
		4）降低硬件成本
    	
```

![1599806917415](/1599806917415.png)

### 2）kubernetes作用：管理容器

https://kubernetes.io/zh/docs/concepts/overview/what-is-kubernetes/ 

```
上面提到了使用容器部署的技术，docker
那就需要kubernetes来管理容器了【可以使用swarm，docker官方的。找到每个服务器上的docker，给每一个docker里面部署一个容器，但是swarm只能管理docker，还有其他的服务器】
	例如：使用swarm启动3个cart服务，swarm会感知集群内多个docker服务器，找到3台来启动cart服务

1、服务发现、负载均衡
	springcloud只是针对java的，它是针对所有服务的
2、存储编排：
	
3、自动部署和回滚：
	如果某服务宕机还可以将这个应用部署到另外一个容器上
	版本不对回滚，自我打包

4、自我修复
	如果宕机了会自动部署到另外一个容器上
5、自动伸缩
	如果超过负载了CPU 占用80，检查到后自动伸缩
```

![1599807906072](/1599807906072.png)



![1599807584739](/1599807584739.png)

### 3）kubernetes组件

[Master 组件](https://v1-17.docs.kubernetes.io/zh/docs/concepts/overview/components/#master-组件) 

[Node 组件](https://v1-17.docs.kubernetes.io/zh/docs/concepts/overview/components/#node-组件)

主从方式：

![1599808744617](/1599808744617.png)

```
master暴露API，命令行或可视化界面调用API，管理NODE，每个NODE上面都安装了docker【镜像仓库】

```

![1599808793994](/1599808793994.png)

#### # master组件

```
master内部还有很多组件
一次流程：
	1）给API Server发送请求，cart部署3台
	2）然后将信息存储到etcd
	3）scheduler从etcd拿到信息开始调度【那个NODE可以完成该任务】
	4）

1、API Server

2、etcd
	存储数据，键值数据库【etcd也可以作为集群】
	
3、kube-scheduler
	
4、controller
	有很多控制器
```

![1599808901921](/1599808901921.png)

![1599809158441](/1599809158441.png)

![1599809093625](/1599809093625.png)

#### # Node组件

![1599809218020](/1599809218020.png)

![1599809244615](/1599809244615.png)

```
kublet：生命周期，代理保证 容器都运行在pod中
kube-proxy：路由器，负载均衡【当前node有多份pod】
```

![1599809406617](/1599809406617.png)

#### # 一次完整调用

```
1、controller监听etcd，会创建pod实例，通过API Server写入etcd
2、scheduler监听到该事件，选定一个落户的Node，将结果通过API Server写入etcd
3、指定的Node的kubelet监听到etcd，创建一个Pod
```

![1599810779911](/1599810779911.png)

![1599810800496](/1599810800496.png)

![1599810811990](/1599810811990.png)

---

![1599810144507](/1599810144507.png)

```
kubectl：可视化界面，给API Server发送请求
pod：例如docker里面多个容器组成一个pod，两个应用组成一个功能

1、kubectl发送请求，master组件API Server接收请求
2、相关命令操作存储到etcd
3、master组件的scheduler根据etcd中存储的信息创建调度任务
4、判断是有状态无状态应用调用不同的controller，例如musql是有状态应用，数据是存在本地的
5、
6、由Node的kubeket创建Pod，Pod内部有多个Container，并且每个Container可以挂载自己的资源
7、多个pod组成一个service，负载均衡
8、访问路由器->kube-proxy

```

![1599809778192](/1599809778192.png)

![1599810167791](/1599810167791.png)

![1599810364342](/1599810364342.png)

![1599810438680](/1599810438680.png)

![1599810509510](/1599810509510.png)

![1599810517648](/1599810517648.png)

![1599810585695](/1599810585695.png)



```
例：指定版本、名字、镜像、端口
```

![1599810604686](/1599810604686.png)



















## 2、使用篇

### 1）6个步骤

```
如何使用？分为以下六步【https://kubernetes.io/zh/里面有下面的图】
	这下面6个是可以点击的，例如1，就会跳进相关教程了：https://kubernetes.io/zh/docs/tutorials/kubernetes-basics/create-cluster/cluster-intro/
```

![1599806693681](/1599806693681.png)

### 2）搭建前的准备工作 P341

```
不使用这个：1、安装minikube
https://github.com/kubernetes/minikube/releases

1、前置要求
一台或多台机器，操作系统CentOS7.x-86_x64
硬件配置:2GB或更多RAM，2个CPU 或更多CPU，硬盘30GB或更多
集群中所有机器之间网络互通
可以访问外网，需要拉取镜像
禁止swap分区I


参照文档：https://v1-17.docs.kubernetes.io/zh/docs/setup/production-environment/
2、kubadmin
kubeadm是官方社区推出的一个用于快速部署kubernetes,集群的工具。这个工具能通过两条指令完成一个kubernetes集群的部署:
	#创建一个Master 节点
	$kubeadm init

	#将一个Node节点加入到当前集群中
	kubeadm join <Master节点的IP和端口>

3、部署步骤
	1.在所有节点上安装Docker和kubeadm
	2.部署 Kubernetes Master
	3.在master上部署容器网络插件
	4.部署 Kubernetes Node，将节点加入Kubernetes集群中【对应小节7】
	5.部署Dashboard Web页面，可视化查看 Kubernetes资源

4、环境准备
	1、准备工作
	我们可以使用vagrant快速创建三个虚拟机。虚拟机启动前先设置virtualbox的主机网络。现全部统一为192.168.56.1，以后所有虚拟机都是56.x的ip地址
	然后全局设置位置，别放C盘
```

![1599811575770](/1599811575770.png)

![1599811911358](/1599811911358.png)





![1599811465234](/1599811465234.png)



### 3）开始搭建

```
1、拷贝k8s文件夹到一个无空格、中文的目录下，修改Vagrantfile的node.vm.box = "centos7"
	因为之前vagrant box add centos7 CentOS-7-x86_64-Vagrant-2004_01.VirtualBox.box给本地设置了一个centos7，否则使用centos/7会去仓库下载会很慢

2、在k8s目录下执行vagrant up，然后就会创建出3个虚拟机

3、修改每个node，使其可以远程连接
	vagrant ssh k8s-node1
	su root   密码：vagrant
	vi /etc/ssh/sshd_config	修改PasswordAuthentication yes
	reboot【service sshd restart】
	xshell连接，192.168.56.100 -- root + vagrant
	
4、修改网络
	网卡1是k8s使用，网卡2仅主机模式是保证主机开发工具可以连接虚拟机【再一个局域网内】
	前言：使用ip route show查看每个虚拟机使用的都是eth0网卡，然后输入ip addr，每个虚拟机的eth0网卡都是10.0.2.215这个ip，原因是都使用的是网络地址转换NAT【三台虚拟机共用一个ip】。
	就是三个虚拟机节点的IP都是一样的，然后通过 + 端口转发【宿主机端口与虚拟机端口的映射（会同时映射3台）】
	1）全部关闭电源
	2）全局设定=》网络=》创建NAT网络
	3）选中node1机器（每个机器都要设置）=》网络=》网卡1=》选择NAT网络（选中刚刚创建的）=》高级=》刷新mac地址【每台虚拟机都会生成一个IP，这个IP要记录下来，ip addr查看网卡1就是eth0的ip】=》ok
	
5、设置linux环境
关闭防火墙:
	systemctl stop firewalld
	systemctl disable firewalld
关闭selinux:【默认安全策略】
	sed -i 's/enforcing/disabled/' /etc/selinux/config
	setenforce 0
关闭swap:
	swapoff -a	临时
	sed -ri 's/.*swap.*/#&/' /etc/fstab
	
修改主机名：hostname 查看主机名如果是localhost，则修改为k8s-node1
hostnamectl set-hostname <newhostname>:指定新的hostname

修改hosts
vi /etc/hosts
10.0.2.15 k8s-node1
10.0.2.4 k8s-node2
10.0.2.5 k8s-node3

将桥接的IPv4流量传递到iptables的链:
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system


疑难问题:
遇见提示是只读的文件系统，运行如下命令
mount -o remount rw /

date查看时间(可选)
yum install -y ntpdate
ntpdate time.windows.com同步最新时间


备份：
给三台主机做备份
```

![1599832767824](/1599832767824.png)

![1599835763117](/1599835763117.png)

### 4）安装Docker、kubeadmin、kubelet、kubectl

#### # 安装docker

```
1、卸载旧版本【uninstall old versions】
	sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
                  
2、安装相关依赖
sudo yum install -y yum-utils device-mapper-persistent-data lvm2

3、设置镜像地址
	sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
推荐使用阿里镜像：
	sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo【这个镜像和后面提到的镜像加速不同，这里指的是下载docker本身的镜像地址】
	
4、安装docker以及docker-cli
sudo yum install -y docker-ce docker-ce-cli containerd.io

5、启动docker
sudo systemctl start docker
虚拟机开机启动：sudo systemctl enable docker

6、测试
docker images -a

7、docker配置镜像下载加速
	1）默认从hub.docker下载软件镜像【很慢】
	2）修改成aliyun的镜像加速器
	3）执行【获取第一步配置的镜像加速器网址】
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://7zi5cb9i.mirror.aliyuncs.com"]
}
EOF

sudo systemctl daemon-reload
sudo systemctl restart docker
	
可以备份一下这里
docker pull quay.io/openebs/provisioner-localpv:1.5.0
```

#### # 添加阿里云yum源

```
cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

#### # 安装kubeadmin、kubelet、kubectl

```
	查看是否安装相关源：yum list|grep kube
1、安装
yum install -y kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3

2、开机启动kubelet，要把每一个Node注册到集群中
sudo systemctl enable kubelet
sudo systemctl start kubelet【到这一步暂时还启动不起来：systemctl status kubelet，后面还有要配置的】


```

#### # kubeadm reset：重置集群

```
https://blog.csdn.net/woay2008/article/details/93250137

swapoff -a
kubeadm reset
主机批量处理：
systemctl daemon-reload
systemctl restart kubelet
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X  
rm -rf $HOME/.kube
主机+从机：
     rm -rf /var/lib/etcd
	 rm -rf /etc/cni/net.d
	 rm -rf /var/lib/kubelet
	 rm -rf /etc/kubernetes

问题解决过程

期间，我尝试了所有能搜索的相关资料，都没有一个好使的。我还确认了kubeadm reset命令会完全清除已创建的集群配置，那么为什么清配置后重新创建集群却不行呢？实在没办法我把注意力集中到额外执行的这几个命令上：

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
这几个命令会创建一个目录，并复制几个配置文件，重新创建集群时，这个目录还是存在的，于是我尝试在执行这几个命令前先执行rm -rf $HOME/.kube命令删除这个目录，最后终于解决了这个问题！！！
总结

这个问题很坑人，删除集群然后重新创建也算是一个常规的操作，如果你在执行 kubeadm reset命令后没有删除创建的 $HOME/.kube目录，重新创建集群就会出现这个问题！
```



### 5）部署k8s-master

#### # master节点初始化

使用kubeadm这个工具

```
1、挑选一台主机为master【当前选择10.0.2.15作为主机】
暂时先不运行下面语句，将k8s文件夹拷贝到master主机的root路径下
sudo kubeadm init \
--apiserver-advertise-address=10.0.2.15 \
--image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
--kubernetes-version v1.17.3 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=10.244.0.0/16

由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。可以手动按照我们的执行master_images.sh先拉取镜像，地址变为registry.aliyuncs.com/google_containers也可以。

科普:无类别域间路由(Classless Inter-Domain Routing、CIDR)是一个用于给用户分配IP地址以及在互联网上有效地路由IP数据包的对IP地址进行归类的方法。
拉取可能失败，需要下载镜像。

运行完成提前复制:加入集群的令牌

pod-network-cidr：docker最小的单位是容器，pod是k8s中最小的单位（多个容器组成）
service-cidr：service是一组pod组成对外提供服务，负载均衡

2、修改可执行文件权限【拉取镜像】
cd k8s
ll查看文件
master_images.sh：rw- 没有执行权限【root没有执行权限】

chmod 700 master_images.sh  => rwx

3、下载镜像：执行master_images.sh
./master_images.sh
docker images 查看镜像下载情况

4、初始化master
kubeadm init \
--apiserver-advertise-address=10.0.2.15 \
--image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
--kubernetes-version v1.17.3 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=10.244.0.0/16

打印Your Kubernetes control-plane has initialized successfully!代表初始化成功

5、 生成配置文件
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

6、部署pod网络
参照doc：https://kubernetes.io/docs/concepts/cluster-administration/addons/
使用Flannel： 是一个可以用于 Kubernetes 的 overlay 网络提供者。

7、记录令牌【这是第4步执行后记录下来的】
kubeadm join 10.0.2.15:6443 --token 1jgtez.7x3xdrn9j15jb1pn \
    --discovery-token-ca-cert-hash sha256:0d6f2f6bd1074a00c6d25b67da8fcbd5bab4ecbefc59fe4a354cb8bc0d1db1cf 

如果token过期，重新获取一个没有过期时间的token【默认是2h过期】：
kubeadm token create --print-join-command --ttl=0
```

### 6）安装pod网络插件（CNI）

```
master配置

1、执行
kubectl apply -f \
https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
这个地址被墙，使用我们k8s文件夹中的yml即可，同时flannel.yml中指定的images访问不到可以去docker hub找一个

2、不执行上述网络地址的yml，使用本地的flannel.yml【先拉取镜像】
	1）如果kube-flannel.yml中指定的images访问不到去docker.hub中查找并修改yml中的配置
		# 1、访问hub.docker.com，搜索flannel，点击from quay.io/coreos/flannel
		# 2、拷贝v0.11.0-amd64的来源jmgao1983/flannel:v0.11.0-amd64
		# 3、flannel.yml中image指定的地方，修改
	2）也可以使用quay-mirror.qiniu.com和registry.aliyuncs.com
	例如quay.io=》quay-mirror.qiniu.com  || gcr.io改为registry.aliyuncs.com
下拉镜像：quay.io/coreos/flannel:v0.10.0-s390x
如果拉取较慢，可以改为：quay-mirror.qiniu.com/coreos/flannel:v0.10.0-s390x

下拉镜像：gcr.io/google_containers/kube-proxy
可以改为： registry.aliyuncs.com/google_containers/kube-proxy

3、执行kubectl apply -f kube-flannel.yml
使用本地的yml规则安装pod组件【kubectl delete -f kube-flannel.yml：删除yml中配置的所有组件 】

4、获取名称空间
kubectl get ns

5、获取所有名称空间的pod
kubectl get pods --all-namespaces -o wide
如果全部都是running就成功了，如果有不是running状态的等待一段时间，下载好镜像后就可以了

docker images|grep flannel可以查到以下镜像：
jmgao1983/flannel                                                             v0.11.0-amd64       ff281650a721        19 months ago       52.6MB

```

访问hub.docker.ccom，搜索flannel：点击from quay.io/coreos/flannel

flannel.yml：修改docker镜像地址，就是下图image指定的那一段。

![1600083281929](/1600083281929.png)

![1600083242226](/1600083242226.png)

### 7）将所有Node节点join都master节点

```
1、获取kubernetes中所有的节点【在master执行】
kubectl get nodes

2、当master是ready状态时，在k8s-node2、k8s-node3执行以下命令将其添加到master
kubeadm join 10.0.2.15:6443 --token 4k7qur.oe6ab1qsgpl7swcm \
    --discovery-token-ca-cert-hash sha256:06b6d21ef7e786b8d759017c29f3041dd886af32026aa5f5333cb91e79f4e636 
    
3、master监控Node各节点
watch kubectl get pod -n kube-system -o wide
查看到node2和node3镜像下载失败
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.17.3
docker pull kubesphere/kube-proxy:v1.17.3
docker pull jmgao1983/flannel:v0.11.0-amd64
```

![1600086458266](/1600086458266.png)

### 8）排除错误方法

```
多学会看日志

1、master执行watch kubectl get pod -n kube-system -o wide发现问题，例如CrashLoopBackOff
https://www.jianshu.com/p/bcc05427990d
	1）找到出问题的主机执行kubectl get pod获取pod的状态
	2）查看pod的详细信息：kubectl describe pod elkhost-944bcbcd4-8n9nj
	3）查看此pod日志：kubectl logs elkhost-944bcbcd4-8n9nj

2、
```

### 9）入门操作kubernetes集群

```
前面已经搭建好了一个kubernetes集群环境，下面的demo是给k8s集群部署一个Tomcat
以前是docker run 下载一个镜像，来启动一个容器，现在是在master节点创建一个部署，指定一个镜像，然后指挥其他节点来下载镜像

1、部署一个tomcat，pod的形式【此时还不能访问没有service】
kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8【1个部署对应1个pod对应1个容器】
Kubectl get pods -o wide可以获取到tomcat信息【Kubectl get all】
# 容灾恢复1：在k8s-node3执行docker stop tomcat6停掉该容器，kubernetes会给node3重启一个容器
# 容灾恢复2：如果k8s-node3宕机，master会找到集群中另一个节点启动pod

2、暴露一个部署【创建service】
kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort
解释：暴露一个tomcat6的部署，容器的8080端口映射pod的80端口，然后使用NodePort的方式【service为pod的80端口随机分配一个端口提供服务】。也可以不使用--type使用--node-port=31533
192.168.56.101:31533访问tomcat6

3、动态扩容测试
kubectl get deployment
应用升级：kubectl set image
扩容哪个部署：kubectl scale --replicas=3 deployment tomcat6
结果：在k8s-node2部署了2个pod，在k8s-node3部署了1个pod，一共3个。并且service对外暴露的接口都是31533
192.168.56.101:31533
192.168.56.102:31533 都可以访问tomcat6
```

![1600091057549](/1600091057549.png)

### 10）命令集合

```
1、kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8
	选择某一节点部署tomcat6，指定镜像，部署后使用Kubectl get pods -o wide可以查到
1、kubectl get pods -o wide
	获取所有pod信息，并且获取到pod部署在哪个节点
   kubectl get pods --namespace kube-system -o wide【获取指定命名空间下的所有pod】
   kubectl --namespace kube-system describe pod tiller-deploy-7b76b656b5-qpmb4【查询pod详细信息】
   kubectl get deployment -n kube-system【查询名称空间下的部署】
   kubectl delete deployment tiller-deploy  --namespace kube-system【删除名称空间下的部署】
	【要删除部署，不要删除pod，否则会自动部署pod】

   kubectl get pod -n kube-system -l app=helm【查看helm】
   kubectl get pods -n kube-system -owide | grep tiller-deploy【查看tiller】
   
3、kubectl get all
	获取所有：pod、service、deployment
4、kubectl get nodes
	获取所有节点
5、kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort
	暴露一个部署。service随机分配端口
6、kubectl get svc
   kubectl get svc -o wide
	查看service，能查看到分配的端口
7、kubectl get deployment
	查看部署
8、kubectl get ns
	查看namespace命名空间
8、kubectl set image
	应用升级
9、kubectl scale --replicas=3 deployment tomcat6
	扩容3份tomcat6这个部署，3个pod
10、kubectl delete deployment.apps/tomcat6-5f654a5s
    kubectl delete service/tomcat6
    kubectl delete pod/tomcat6-5f654a5s-mlntp
    删除
11、watch kubectl get pod -n kube-system -o wide
	监控
	
12、查看日志：kubectl logs --namespace kubesphere-devops-system

```

![1600092795688](/1600092795688.png)

![1600093091103](/1600093091103.png)

### 11）污点

```
https://docs.lvrui.io/2018/11/14/%E4%B8%BAk8s-master%E8%8A%82%E7%82%B9%E6%B7%BB%E5%8A%A0%E6%B1%A1%E7%82%B9taints/
```



## 3、k8s细节

### 1）kubectl命令

```
https://kubernetes.io/zh/docs/reference/kubectl/overview/
查看文档
```

#### # kubectl文档

#### # 资源类型

#### # 格式化输出

#### # 常用操作

#### # 命令参考





### 2）yaml语法

```
之前是使用命令的方式 创建 部署、暴露部署、创建pod【多个容器】

```



```
之前使用kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8命令行的方式部署tomcat6，现在只需要kubectl apply -f example-controller.yaml就可以实现一次部署了

1、使用yaml的方式完成部署
	1）生成一段yaml模板到tomcat6.yaml
	kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8 --help可以看到--dry-run -o yaml生成模板的提示
	生成模板到文件tomcat6.yaml：
	kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8 --dry-run -o yaml > tomcat6.yaml
	2）使用该yaml完成部署
	kubectl apply -f tomcat6.yaml
	然后kubectl get pods，查看到部署的pods
	
2、使用yaml的方式完成暴露部署
	1）生成一段yaml模板到tomcat6_expose.yaml
	kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort --dry-run -o yaml
	kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort --dry-run -o yaml > tomcat6_expose.yaml
	2）使用该yaml完成暴露
	kubectl apply -f tomcat6_expose.yaml
	
3、使用yaml的方式创建pod
	kubectl get pod tomcat6-5f7ccf4cb9-kb5hv -o yaml > mypod.yaml
如下图，创建两个image。
	kubectl apply -f mypod.yaml
	kubectl get pods : 0/2 获取到tomcat6-new 有2个容器
	

```

![1600095450915](/1600095450915.png)

![1600095496466](/1600095496466.png)

#### # yml模板

![1600093981351](/1600093981351.png)

#### # yaml字段解析



### 3）入门操作

#### # pod、controller、service

```
Pod是Kubernetes应用程序的基本执行单元，即它是Kubernetes对象模型中创建或部署的最小和最简单的单元。Pod表示在集群上运行的进程。

控制器：创建和管理POD以何种方式部署【副本、上线、修复能力】
	ReplicaSet：取代了ReplicationController，用于副本复制
	ReplicationController：过时
	Deployments：
	StatefulSets：有状态的部署【mysql有状态部署】
	DaemonSet：每一个节点都需要启动

service：
	service是对pod的负载均衡，pod可以使用不同的端口，然后整个service提供一个单独的端口为外接提供服务。


```

controller和service：

![1600254533803](/1600254533803.png)



# 二、KubeSphere

## 1、安装前提环境

```
KubeSphere是一个可视化工具
前提需要安装：https://www.cnblogs.com/xiao987334176/p/13267339.html
	1、helm+tiller
	2、存储类型StorageClass【这里安装的是OpenEBS，上面网站教程安装的是nfs】
	3、安装部署KubeSphere【需要8核16g，这里安装的是mini版本】

KubeSphere是一款面向云原生设计的开源项目，在目前主流容器调度平台 Kubernetes之上构建的分布式多租户容器管理平台，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大降低开发、测试、运维的日常工作的复杂度。

安装网址doc：https://v2-1.docs.kubesphere.io/docs/zh-CN/installation/prerequisites/

```

![1600255046194](/1600255046194.png)

### 1、安装helm+tiller

```

KubeSphere的doc：https://v2-1.docs.kubesphere.io/docs/zh-CN/installation/prerequisites/

下载helm参照：https://blog.csdn.net/qianghaohao/article/details/98851147【被墙】
helm安装指南：https://zhuanlan.zhihu.com/p/77496043

1、安装Helm （master节点执行）
这个网站有很多中下载heml的方法：http://www.coderdocument.com/docs/helm/v2/using_helm/installing_helm.html

方法一：
	1）使用sh文件下载【需要翻墙】：curl -L https://git.io/get_helm.sh | bash
	2）chmod 700 get_helm.sh
	3）./get_helm.sh

方法二：
	1）直接去github下载tar.gz：https://github.com/helm/helm/tags
	2）然后解压 tar -zxvf helm-v2.16.3-linux-amd64.tar.gz
	3）mv /root/k8s/linux-amd64/helm /usr/local/bin/helm
	   mv /root/k8s/linux-amd64/tiller /usr/local/bin/tiller
	4）helm help【查看是否安装成功】

2、创建服务账号和角色绑定
vi helm-rbac.yaml，把下面内容加到yaml中

apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system

3、执行：kubectl apply -f helm-rbac.yaml

4、、初始化+安装tiller【这里最终解决是修改了dns】
	helm init --service-account tiller --upgrade -i registry.aliyuncs.com/google_containers/tiller:v2.16.3
 	【--tiller-image指定镜像，否则会被墙】
安装tiller的doc：https://zhuanlan.zhihu.com/p/77496043

5、检查tiller安装是否成功【如果安装失败，参考命令集合，查看pod细节 describe】
【这四个都可以查是否安装成功】
tiller
helm version
kubectl get deployment -n kube-system【看pod】
kubectl get pod -n kube-system -l app=helm【看部署】
kubectl get pods --all-namespaces【看所有pods】
```

```
修改dns：【解决docker拉取失败】
vim /etc/resolv.conf
nameserver 114.114.114.114将原来的覆盖掉
不需要重启网络，否则会将新的dns覆盖掉

也可以在centos7中永久性的修改dns，但感觉速度不是很好
cd /etc/sysconfig/network-scripts/
ls
打开设置了静态ip的网卡文件，如ifcfg-enp0s8

在PEERDNS="no"下面添加
DNS1=8.8.8.8
DNS2=114.114.114.114
【
PEERDNS="no"
DNS1=8.8.8.8
DNS2=114.114.114.114
】

重启网络
sudo service network  restart

查看dns是否修改
cat /etc/resolv.conf
```

### 2、安装 存储类型 OpenEBS （ StorageClass ）

```
OpenEBS 只是kubernetes的一种存储类型，还有GlusterFS、NFS（network file system ）

```

```
下载教程：doc：https://v2-1.docs.kubesphere.io/docs/zh-CN/appendix/install-openebs/
1、查看节点名称：
	kubectl get node -o wide
2、确认 master 节点是否有 Taint，如下看到 master 节点有 Taint。
	kubectl describe node k8s-node1 | grep Taint
3、删除 master 节点的 Taint：【污点，就是不让master可以部署。去掉后就可以部署了】
	
	kubectl taint nodes k8s-node1 node-role.kubernetes.io/master:NoSchedule-
4、安装 OpenEBS【http://docs.openebs.io/】
	1）创建 OpenEBS 的 namespace，OpenEBS 相关资源将创建在这个 namespace 下：
	kubectl create ns openebs
	kubectl get ns
	2）安装
	helm install --namespace openebs --name openebs stable/openebs --version 1.5.0
	
	BUG1：
	helm repo remove stable
	helm repo add stable https://kubernetes-charts.storage.googleapis.com
	helm repo update
	helm search
	然后重新安装：
	helm install --namespace openebs --name openebs stable/openebs --version 1.5.0
	
	BUG2：镜像下载失败【重新设置一下阿里云加速】
	查看部署
	kubectl get deployment -n openebs
	kubectl --namespace openebs describe pod openebs-ndm-g8mfh
	再下载image【k8s-node2、k8s-node3都下载】
	docker pull openebs/admission-server:1.5.0			
	docker pull openebs/m-apiserver:1.5.0      			
	docker pull openebs/provisioner-localpv:1.5.0		
	docker pull openebs/node-disk-manager-amd64:v0.4.5  【这个在k8s-node1（master）中下载】
	docker pull openebs/node-disk-operator-amd64:v0.4.5 
	docker pull openebs/openebs-k8s-provisioner:1.5.0	
	
	docker pull openebs/snapshot-controller:1.5.0		
	docker pull openebs/snapshot-provisioner:1.5.0		
	然后查看部署状态：【全部running，则安装成功】
	kubectl get deployment -n openebs
	watch kubectl get pods --all-namespaces -o wide【监控，如果全部ready则成功】
	
5、安装 OpenEBS 后将自动创建 4 个 StorageClass，查看创建的 StorageClass
	kubectl get sc
6、将 openebs-hostpath设置为 默认的 StorageClass：
kubectl patch storageclass openebs-hostpath -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

7、OpenEBS 的 LocalPV 已作为默认的存储类型创建成功。可以通过命令 kubectl get pod -n openebs来查看 OpenEBS 相关 Pod 的状态，若 Pod 的状态都是 running，则说明存储安装成功
【污点在kubesphere安装成功后加】
kubectl taint nodes k8s-node1 node-role.kubernetes.io/master=:NoSchedule
```



![1600273465679](/1600273465679.png)

## 2、yaml部署+暴露tomcat6

```
之前是命令行的方式安装，现在使用yaml
1、创建yaml
kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8 --dry-run -o yaml > tomcat6-deployment.yaml
2、修改yaml，3个副本
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: tomcat6
  name: tomcat6
spec:
  replicas: 3
  selector:
    matchLabels:
      app: tomcat6
  template:
    metadata:
      labels:
        app: tomcat6
    spec:
      containers:
      - image: tomcat:6.0.53-jre8
        name: tomcat
3、部署
kubectl apply -f tomcat6-deployment.yaml 
kubectl get all：查看部署效果
kubectl get pods --all-namespaces -o wide：查看pods

4、暴露【暴露与部署可以一次执行】
	1）运行以下命令，并复制生成的yaml代码
	kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort --dry-run -o yaml
	2）修改部署文件，在文档末位加上---，然后另起一行将以下内容粘贴进去
	vi tomcat6-deployment.yaml 
	
apiVersion: v1
kind: Service
metadata:
  labels:
    app: tomcat6
  name: tomcat6
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: tomcat6
  type: NodePort

5、删除之前的部署
kubectl get all
kubectl delete deployment.apps/tomcat6

6、部署+暴露
kubectl apply -f tomcat6-deployment.yaml 
kubectl get all

7、访问
192.168.56.101:32501
192.168.56.102:32501
```

![1600321201191](/1600321201191.png)

## 3、安装ingress

![1600321387857](/1600321387857.png)

```
ingress底层就是nginx做的，可以负载均衡到一个service，直接访问域名而不需要使用ip访问了
192.168.56.101:32501
192.168.56.102:32501

查看ingress-controller，使用的controller是DaemonSet，所以每个节点都要部署

1、kubectl apply -f ingress-controller.yaml 【每一个节点都要部署】

2、创建ingress规则
注意：servicePort是暴露的pod端口，而不是service暴露的随机端口
vi ingress-tomcat6.yaml

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: web
spec:
  rules:
  - host: tomcat6.atguigu.com
    http:
      paths:
        - backend:
           serviceName: tomcat6
           servicePort: 80
3、kubectl apply -f ingress-tomcat6.yaml

4、宿主机配置host
192.168.56.102 tomcat6.atguigu.com

5、访问，就可以直接访问tomcat6了，ingress监听80端口转发到102上面的8080pod上
http://tomcat6.atguigu.com/

如果此时192.168.56.102 node3节点宕机，请求会被转发到node2192.168.56.101上
```

使用DaemonSet，每一个节点都要部署：

![1600322453986](/1600322453986.png)



## 4、安装KubeSphere

```
最关键的两个文档：
https://kubesphere.com.cn/forum/d/1262-k8s/56【安装OpenEBS和KubeSphere都要删掉master的污点】
https://kubesphere.com.cn/forum/d/1272-kubeadm-k8s-kubesphere-2-1-1【安装nfs版本】

这是guthub的安装指南：https://github.com/kubesphere/ks-installer/blob/master/README_zh.md
中国官方安装文档：https://v2-1.docs.kubesphere.io/docs/zh-CN/installation/install-on-k8s/
```

![1600359650270](/1600359650270.png)

```
【这个是基本的安装流程，具体流程需要看后面一个版块的流程】

1、下载yaml【下载不到】
安装mini版本：
wget https://raw.githubusercontent.com/kubesphere/ks-installer/v3.0.0/deploy/kubesphere-installer.yaml
wget https://raw.githubusercontent.com/kubesphere/ks-installer/v3.0.0/deploy/cluster-configuration.yaml

2、查看日志
kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='{.items[0].metadata.name}') -f

3、访问：192.168.56.100:30880/login
admin/P@88w0rd
改成了Wan123
```

```
这个版本是添加和关闭相关组件的版本：
1、下载yaml
wget https://raw.githubusercontent.com/kubesphere/ks-installer/v3.0.0/deploy/kubesphere-installer.yaml
wget https://raw.githubusercontent.com/kubesphere/ks-installer/v3.0.0/deploy/cluster-configuration.yaml

2、调整虚拟机内存
Node2和Node调到11G，6核

3、安装
kubectl apply -f kubesphere-installer.yaml
kubectl apply -f cluster-configuration.yaml

4、查看日志
kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='{.items[0].metadata.name}') -f

5、安装可插拔功能组件
kubectl edit cc -n kubesphere-system ks-installer

kubectl edit cm -n kubesphere-system ks-installer【之前试了这个没用】
kubectl edit cm -n kubesphere-system  ks-console-config
6、设置污点
kubectl taint nodes k8s-node1 node-role.kubernetes.io/master=:NoSchedule
```

![1600340780958](/1600340780958.png)

```
这是下载到本地的日志：
1、关闭应用商店
2、开启DevOps系统【Jenkins一站式部署】
3、开启KubeSphere日志系统
4、关闭Service Mesh微服务治理功能【熔断、限流、链路追踪】【前提是要开启日志】
5、开启KubeSphere告警通知系统【】
6、安装Metrics-Server开启HPA【监控CPU使用率 弹性伸缩】
```

```
kubesphere-minimal.yaml

---
apiVersion: v1
kind: Namespace
metadata:
  name: kubesphere-system

---
apiVersion: v1
data:
  ks-config.yaml: |
    ---

    persistence:
      storageClass: ""

    etcd:
      monitoring: False
      endpointIps: 192.168.0.7,192.168.0.8,192.168.0.9
      port: 2379
      tlsEnable: True

    common:
      mysqlVolumeSize: 20Gi
      minioVolumeSize: 20Gi
      etcdVolumeSize: 20Gi
      openldapVolumeSize: 2Gi
      redisVolumSize: 2Gi

    metrics_server:
      enabled: False

    console:
      enableMultiLogin: False  # enable/disable multi login
      port: 30880

    monitoring:
      prometheusReplicas: 1
      prometheusMemoryRequest: 400Mi
      prometheusVolumeSize: 20Gi
      grafana:
        enabled: False

    logging:
      enabled: False
      elasticsearchMasterReplicas: 1
      elasticsearchDataReplicas: 1
      logsidecarReplicas: 2
      elasticsearchMasterVolumeSize: 4Gi
      elasticsearchDataVolumeSize: 20Gi
      logMaxAge: 7
      elkPrefix: logstash
      containersLogMountedPath: ""
      kibana:
        enabled: False

    openpitrix:
      enabled: False

    devops:
      enabled: True
      jenkinsMemoryLim: 2Gi
      jenkinsMemoryReq: 1500Mi
      jenkinsVolumeSize: 8Gi
      jenkinsJavaOpts_Xms: 512m
      jenkinsJavaOpts_Xmx: 512m
      jenkinsJavaOpts_MaxRAM: 2g
      sonarqube:
        enabled: True
        postgresqlVolumeSize: 8Gi

    servicemesh:
      enabled: False

    notification:
      enabled: True

    alerting:
      enabled: True

kind: ConfigMap
metadata:
  name: ks-installer
  namespace: kubesphere-system

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ks-installer
  namespace: kubesphere-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: null
  name: ks-installer
rules:
- apiGroups:
  - ""
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - apps
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - extensions
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - batch
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - apiregistration.k8s.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - tenant.kubesphere.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - certificates.k8s.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - devops.kubesphere.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - monitoring.coreos.com
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - logging.kubesphere.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - jaegertracing.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - storage.k8s.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - '*'
  verbs:
  - '*'

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ks-installer
subjects:
- kind: ServiceAccount
  name: ks-installer
  namespace: kubesphere-system
roleRef:
  kind: ClusterRole
  name: ks-installer
  apiGroup: rbac.authorization.k8s.io

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ks-installer
  namespace: kubesphere-system
  labels:
    app: ks-install
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ks-install
  template:
    metadata:
      labels:
        app: ks-install
    spec:
      serviceAccountName: ks-installer
      containers:
      - name: installer
        image: kubesphere/ks-installer:v2.1.1
        imagePullPolicy: "Always"
```

## 5、 KubeSphere v3.0.0对接SonarQube 

```
https://kubesphere.com.cn/forum/d/2044-kubesphere-v3-0-0-sonarqube

1、不同版本语句不同，具体使用 --help查看
helm upgrade --install sonarqube sonarqube --repo https://charts.kubesphere.io/main --namespace kubesphere-devops-system --set service.type=NodePort

helm upgrade --install sonarqube sonarqube --repo https://charts.kubesphere.io/main -n kubesphere-devops-system  --create-namespace --set service.type=NodePort

2、根据提示获取sonarqube登录信息
  export NODE_PORT=$(kubectl get --namespace kubesphere-devops-system -o jsonpath="{.spec.ports[0].nodePort}" services sonarqube-sonarqube)
  export NODE_IP=$(kubectl get nodes --namespace kubesphere-devops-system -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
 
3、获取sonarqube token
登录sonarqube
user: admin
password: admin

My Account –> Security –> Tokens
填入token名称【gulimall-analyze】，点击Generate创建token
记录token值，备用【后面凭证那块需要】

83826e0a0371fadd0d76e5c8dfbd8a2995d6ef09

4、选择continue按钮，但是没找着
要选择java项目、maven构建

```

![1600583506178](/1600583506178.png)

## 6、多租户管理快速入门

```
中文官方文档说明：https://v2-1.docs.kubesphere.io/docs/zh-CN/quick-start/admin-quick-start/
平台资源分为三级：集群 (Cluster)、 企业空间 (Workspace)、 项目 (Project) 和 DevOps Project (DevOps 工程)
	集群 (Cluster)：Node、service这些都是集群资源【不可以所有人能访问】
	企业空间 (Workspace)：分为不同的企业空间
	项目 (Project) 和 DevOps Project (DevOps 工程)：部署在企业空间上的项目【所以需要自己的企业空间】
```

![1600493263143](/1600493263143.png)

```
内置角色：
platform-admin				平台管理员，可以管理平台内的所有资源。【admin账户】
只分配以下三个角色：
users-manager				平台用户管理员，管理平台所有用户。【只能创建用户】
workspaces-manager			平台企业空间管理员，管理平台所有企业空间。【只能创建企业空间】
platform-regular			平台普通用户，在被邀请加入企业空间或集群之前没有任何资源操作权限。【有了这个角色就可以使用资源了】

1、创建角色管理用户
	1）创建角色
		3.0有默认的users-manager角色所以这里省略了这一步
	2）创建用户
		用户名：atguigu-hr
		邮箱：atguigu-hr@163.com
		角色：users-manager
		密码：Wan123
2、登录atguigu-hr创建用户
	1）创建工作空间管理员ws-manager，role：workspaces-manager
		作用：创建企业空间+分配企业空间管理员：ws-admin
	2）创建指定企业空间管理员ws-admin，role：platform-regular
		作用：邀请企业空间成员+分配成员权限
	3）创建项目管理员project-admin，role：platform-regular
		作用：创建项目+邀请项目成员+分配成员权限
	4）创建普通项目成员project-regular，role：platform-regular
		作用：创建部署、密钥、存储pvc

3、登录ws-manager创建企业空间
	1）创建企业空间gulimall-workspace
	2）指派管理员ws-admin
	
4、登录ws-admin邀请企业成员
	1）project-admin：可以创建项目	 role：gulimall-workspace-self-provisioner
	2）project-regular：只能查看项目 role：gulimall-workspace-viewer
	
5、登录project-admin创建项目【测试】
	测试1）创建资源型项目gulimall，并邀请project-regular为该项目的开发人员
	测试2）创建DevOps项目

```

创建企业空间：

![1600506501483](/1600506501483.png)

登录ws-admin邀请企业成员：

![1600507256731](/1600507256731.png)



## 7、创建 Wordpress 应用并发布到k8s【普通的部署：应用】

```
参照文档：https://v2-1.docs.kubesphere.io/docs/zh-CN/quick-start/wordpress-deployment/
下面就不记笔记了

1、直接使用docker部署。
	1）拉取好mysql镜像，然后结合Wordpress部署连接mysql就可以启动应用了
	
2、使用kubernetes部署
	1）前端先创建Wordpress的deployment，部署就会下载镜像生成pod，然后创建service对外暴露【service的好处就是跟集群kubernetes其他节点网络是互通的（负载）】
	2）后端创建mysql的deployment，生成pod，创建service对外暴露
	3）secret：密钥，不是明文传输，当需要使用的时候直接引入
	4）PV：节点的网络存储，当一个存储请求过来了，会绑定该请求进行存储
	5）pvc（persistent Volume Claim）：存储请求【前端Wordpress的静态资源，mysql对应的磁盘文件】
```

![1600508732458](/1600508732458.png)

![1600508566712](/1600508566712.png)

### 1）创建 企业空间、项目、应用

```
1、登录ws-manager创建企业空间wordpress，指派ws-admin作为该企业空间的管理员【gulimall-workspace-admin】

2、登录ws-admin邀请人员进入该企业空间，指定project-admin为项目管理者【可以创建项目gulimall-workspace-self-provisioner】，project-regular【不可以创建项目gulimall-workspace-viewer】

3、登录project-admin创建wordpress项目，指定人员project-regular为开发人员

4、登录project-regular进入项目，创建应用
```

### 2）创建密钥

```
1、创建mysql 密钥
也可以创建yaml文件，然后kubectl apply -f xxx.yaml

kind: Secret
apiVersion: v1
metadata:
  name: mysql-secret
  namespace: wordpress-project
  annotations:
    kubesphere.io/alias-name: MySQL 密钥
    kubesphere.io/creator: project-regular
    kubesphere.io/description: MySQL 初始密码 123456
data:
  MYSQL_ROOT_PASSWORD: MTIzNDU2
type: Opaque





2、创建WordPress 密钥




```



### 3）创建存储卷

```
创建两个存储卷，wordpress和mysql各一个
wordpress-pvc
mysql-pvc
```

![1600511962914](/1600511962914.png)

### 4）创建应用

![1600512817453](/1600512817453.png)

mysql镜像环境变量关联之前配置的root密钥

![1600512961518](/1600512961518.png)

```
挂载存储卷：
docker run name 5ome-mysqlv /my/oun/datadir:/var/lib/mysql -e WSQl_ROOT_PASSMORD=mly-secret-pw -d mysql:tag

以前会将/var/lib/mysql容器内部资源挂载到本地linux系统下，同样适用kubernetes部署也要将/var/lib/mysql这个目录挂载到pvc里面
```



### 5）查看应用资源

### 6）访问 Wordpress

# 三、DevOps

## 1、设么是DevOps

```
可持续部署、持续集成
DEV：怎么开发？【高并发：怎么承担高并发】
OPS：怎么部署？【高可用：怎么做到高可用】

DevOps是 开发、测试、运维 三者叠加 都需要了解的内容【了解对方的通电难点，合理设计】
DevOps的作用：软件产品交付过程中IT工具链的打通，使各个团队减少时间损耗、更高效地协同工作

```

![1600521932785](/1600521932785.png)

## 2、CI和CD

![1600522213101](/1600522213101.png)

### 1）持续集成(Continuous Integration)

```
集成：将所有代码打包继承到整个项目中，然后测试
而持续集成需要具备的条件是以下5条
	
	1、持续集成是指软件个人研发的部分向软件整体部分交付,频繁进行集成以便更快地发现其中的错误。"持续集成"源自于极限编程(XP），是XP最初的12种实践之一。
	2、CI需要具备这些:
		1）全面的自动化测试。这是实践持续集成&持续部署的基础，同时，选择合适的自动化测试工具也极其重要;
		2）灵活的基础设施。容器，虚拟机的存在让开发人员和QA人员不必再大费周折;
		3）版本控制工具。如 Git.CVS，SVN等;
		4）自动化的构建和软件发布流程的工具，如Jenkins.flow.ci;
		5）反馈机制。如构建/测试的失败，可以快速地反馈到相关负责人，以尽快解决达到一个更稳定的版本。
```

### 2）持续交付(Continuous Delivery)

```
交付：将测试通过的代码交付到 类生产环境

	持续交付在持续集成的基础上,将集成后的代码部署到更贴近真实运行环境的「类生产环境」(production-like environments）中。持续交付优先于整个产品生命周期的软件部署，建立在高水平自动化持续集成之上。
【灰度发布。】
持续交付和持续集成的优点非常相似:
	1、快速发布。能够应对业务需求，并更快地实现软件价值。
	2、编码->测试->上线->交付的频繁迭代周期缩短，同时获得迅速反馈;
	3、高质量的软件发布标准。整个交付过程标准化、可重复、可靠.
	4、整个交付过程进度可视化，方便团队人员了解项目成熟度;
	5、更先进的团队协作方式。从需求分析、产品的用户体验到交互设计、开发、测试、运维等角色密切协作，相比于传统的瀑布式软件团队，更少浪费。


```

### 3）持续部署(Continuous Deployment)

```
	持续部署是指当交付的代码通过评审之后，白动部署到生产环境中。持续部署是持续交付的最高阶段。这意味着，所有通过了一系列的自动化测试的改动都将自动部署到生产环境。它也可以被称为"Continuous Release"。
	
		“开发人员提交代码，持续集成服务器获取代码，执行单元测试，根据测
		试结果决定是否部署到预演环境，如果成功部署到预演环境，进行整体
		验收测试，如果测试通过，自动部署到产品环境，全程自动化高效运转。”
	
	【持续部署主要好处是，可以相对独立地部署新的功能，并能快速地收集真实用户的反馈。】
		"You build ityou run it，这是Amazon一年可以完成 5000万次部署，
		平均每个工程师每天部署超过50次的核心秘籍。
```

Jams Bowman绘制的持续交付工具链图  P357

![1600523044067](/1600523044067.png)

## 3、流水线流程

```
1、checkout scm：从github拉取代码
2、unit test：单元测试
3、sonarqube analysis：代码审查
4、build and push snapshot：构建镜像推送到docker hub中【登录docker hub查看tag】
5、push the latest image：推送最新镜像【docker hub最新镜像被覆盖】
6、deploy to dev：手动确认是否部署到开发环境【namespace一样的项目】
7、push the tag：手动确认release代码是否推送到github的tag上【git tag -a v0.0.2 -m v0.0.2】同时0.0.2也会推送到docker hub上，所以可能会存在两份一样的
8、deploy to production：是否部署到生产环境【namespace一样的项目】

1、开发环境项目，从github拉取版本创建容器部署
2、生产环境项目，从github拉取版本创建容器部署
```

![1600524033548](/1600524033548.png)

![1600523932369](/1600523932369.png)

## 4、创建流水线【demo】

### 1）创建凭证+fork项目+修改Jenkinsfile

```
https://v2-1.docs.kubesphere.io/docs/zh-CN/quick-start/devops-online/
https://www.kubernetes.org.cn/5826.html
需要一个Jenkinsfile ，记录三个凭证
	DockerHub、GitHub 和 kubeconfig (kubeconfig 用于访问接入正在运行的 Kubernetes 集群)
	
1、创建凭证【四个凭证，还有之前记录的SonarQube的token】
	dockerhub-id：账户凭证
	github-id：账户凭证
	demo-kubeconfig：kubeconfig
	【代码审查】sonar-qube：秘密文本，密钥填入token值
	这个是内置的：https://v2-1.docs.kubesphere.io/docs/zh-CN/devops/sonarqube
	访问路径：kubectl get all -n wordpress-project
	我使用的是3.0版本自己下载的pod【查看之前的章节】
	
2、将项目fork到自己的仓库
https://github.com/kubesphere/devops-java-sample

3、修改Jenkinsfile-online
    environment {
        DOCKER_CREDENTIAL_ID = 'dockerhub-id'
        GITHUB_CREDENTIAL_ID = 'github-id'
        KUBECONFIG_CREDENTIAL_ID = 'demo-kubeconfig'
        REGISTRY = 'docker.io'
        DOCKERHUB_NAMESPACE = 'wanzenghui'
        GITHUB_ACCOUNT = 'wanzenghui'
        APP_NAME = 'devops-java-sample'
        SONAR_CREDENTIAL_ID = 'sonar-qube'
    }

4、删除Jenkinsfile-online中maven的-o


```

![1600525306457](/1600525306457.png)

![1600573773919](/1600573773919.png)

### 2）创建 DevOps工程、流水线

```
其实 项目=DevOps工程，应用=流水线

1、找到devops-java-sample/deploy/dev/devops-sample-svc.yaml，看任意一个yaml里面部署的名称空间，这个demo有两个项目两个名称空间，kubesphere-sample-dev和 kubesphere-sample-prod

	1）登录project-admin，创建两个资源型项目【kubesphere-sample-dev和 kubesphere-sample-prod】并邀请成员project-regular作为operator
	2）登录project-admin，创建一个DevOps工程，邀请成员project-regular作为operator
	
2、创建流水线
	1）登录project-regular，进入DevOps工程，创建一个流水线demo-cicd
	2）选择一个代码仓库github=》获取token，然后选择自己的项目。下一步
	3）丢弃分支，使用默认值
	4）指定路径，jenkinsfile-online
	5）设置拉取时间5min，和webhook回调【在github设置】
	
3、创建完毕，开始拉取项目、依赖、构建、单元测试
	流水线创建完成后会创建两个活动，因为有两个分支master+dependency
	1）此时会弹出异常，原因：拉取jar包失败
		点开活动 master，查看日志：从repo.spring.io/spring-boot-starter-parent下载失败，应该从maven仓库去下载
	2）解决bug，停止master活动
	修改代码pom.xml中spring-boot-starter-parent的版本【去spring.io查看springboot的版本】，删除自定义的maven仓库<repositories>
	然后运行流水线，tag_name设置v0.0.2
```

登录project-admin创建两个项目

![1600583974027](/1600583974027.png)



设置webhook回调：

![1600585436057](/1600585436057.png)

![1600585373321](/1600585373321.png)





# 四、集群、分库分表

## 集群的目标

```
集群的目标
	1、高可用(High Availability)，是当一台服务器停止服务后，对于业务及用户毫无影响。停止服务的原因可能由于网卡、路由器、机房、CPU负载过高、内存溢出、自然灾害等不可预期的原因导致，在很多时候也称单点问题。【双主、raft选举】
	2、突破数据量限制，一台服务器不能储存大量数据，需要多台分担，每个存储一部分，共同存储完整个集群数据。最好能做到互相备份，即使单节点故障，也能在其他节点找到数据。【分片】
	3、数据备份容灾，单点故障后，存储的数据仍然可以在别的地方拉起。【主从复制】
	4、压力分担，由于多个服务器都能完成各自一部分工作，所以尽量的避免了单点压力的存在【从机，读写分离】

分库分表：
	这个不是集群，因为磁盘可以加到很大。但是如果单张表非常大，会导致索引效率下降
```

## 集群的基础形式

```
一、主从式
	1、主从复制：同步方式【从节点复制主节点的数据保证一样，从机读，主机写】
		优点：压力分担
		demo：innodb cluster
	2、主从调度：控制方式
		缺点：主机宕机没有调度了
		demo：k8s，master节点调度选择一个Node服务
	
二、分片式【客户端调度，代码逻辑判断调度的节点而不是服务端调度，所以没有主机宕机危险】
	1、数据分片存储，片区之间备份。
		优点：突破数据量限制
	demo：sharding proxy【还实现了读写分离】

三、选主式：raft算法
	为了容灾选主：
	为了调度选主：选出一个主节点作为调度【k8s】
	
	demo：redis的哨兵
```

![1600587951969](/1600587951969.png)

## 1、mysql集群【主从复制】

```
原理与实现：https://segmentfault.com/a/1190000008942618
主+从
```



### 1）集群方案【原理】

```
1、双主复制+主从复制+读写分离：高可用、容灾、压力分担

2、分片：突破数据量限制【就是多套 方案1，各自范围内，由一个节点控制最终请求到达哪套 主从节点】

```

![1600590072553](/1600590072553.png)

#### # 候选主节点

##### 1.1、双主复制-MMM

```
主主复制管理器
1、两个主机master1，master2，其中master1作为write vip，两个主机 数据复制。当第一个主机宕机，第二个主机晋升为主机，切客户端的使用的mysql写ip不用改变，monitor会将写ip的地址转换给master2【ip飘移】
2、master2和slave作为读
3、monitor：监控器，监控IP集。
4、VIP集：虚拟ip集，每个结点都有自己的IP。

优点：高可用问题
缺点：数据丢失【例如master1宕机还没来得及复制的数据】
```

![1600590959582](/1600590959582.png)

##### 1.2、从节点替补-MHA

```
MHA，太慢了，使用从节点作为替补主节点
```

![1600591934243](/1600591934243.png)

##### 1.3、InnoDB Cluster

![1600592086139](/1600592086139.png)

```
MySQL Router：调度节点，如果Primary宕机，选择一个作为主节点
```



![1600592392335](/1600592392335.png)

### 2）集群实现

#### # 实现主从复制【InnoDB Cluster】

```
InnoDB Cluster有单主模式、双主模式

docker 安装模拟Mysql 主从复制 集群
1、下载mysql镜像

2、创建Master实例并启动
docker run -p 3307:3306 --name mysql-master \
-v /mydata/mysql/master/log:/var/log/mysql \
-v /mydata/mysql/master/data:/var/lib/mysql \
-v /mydata/mysql/master/conf:/etc/mysql \
-e MYSQL_ROOT_PASSWORD=root \
-d mysql:5.7

参数说明
-p 3307:3306:将容器的3306端口映射到主机的3307端口
-v/mydata/mysal/master/conf:./etc/mysql:将配置文件夹挂在到主机
-v/mydata/mysal/master/log:/var/log/mysal:将日志文件夹挂载到主机
-v/mydata/mysal/master/data:/ar/lib/mysal:将配置文件夹挂载到主机
-e MYSQL_ROOT_PASSWORD=root:初始化root用户的密码

3、创建slave实例并启动
docker run -p 3317:3306 --name mysql-slaver-01 \
-v /mydata/mysql/slaver/log:/var/log/mysql \
-v /mydata/mysql/slaver/data:/var/lib/mysql \
-v /mydata/mysql/slaver/conf:/etc/mysql \
-e MYSQL_ROOT_PASSWORD=root \
-d mysql:5.7

4、修改master配置
vi /mydata/mysql/master/conf/my.cnf
注意：skip-name-resolve一定要加，不然连接mysql会很慢

[client]
default-character-set=utf8

[mysql]
default-character-set=utf8

[mysqld]
init_connect='SET collation_connection = utf8_unicode_ci'
init_connect='SET NAMES utf8'
character-set-server=utf8
collation-server=utf8_unicode_ci
skip-character-set-client-handshake
skip-name-resolve

server-id=1
log-bin=mysql-bin
read-only=0
binlog-do-db=gulimall_oms
binlog-do-db=gulimall_pms
binlog-do-db=gulimall_sms
binlog-do-db=gulimall_ums
binlog-do-db=gulimall_wms
binlog-do-db=gulimall_admin

replicate-ignore-db=mysql
replicate-ignore-db=sys
replicate-ignore-db=performance_schema
replicate-ignore-db=information_schema

5、修改salver配置
vi /mydata/mysql/slaver/conf/my.cnf
注意：skip-name-resolve一定要加，不然连接mysql会很慢

[client]
default-character-set=utf8

[mysql]
default-character-set=utf8

[mysqld]
init_connect='SET collation_connection = utf8_unicode_ci'
init_connect='SET NAMES utf8'
character-set-server=utf8
collation-server=utf8_unicode_ci
skip-character-set-client-handshake
skip-name-resolve

server-id=2
log-bin=mysql-bin
read-only=1
binlog-do-db=gulimall_oms
binlog-do-db=gulimall_pms
binlog-do-db=gulimall_sms
binlog-do-db=gulimall_ums
binlog-do-db=gulimall_wms
binlog-do-db=gulimall_admin

replicate-ignore-db=mysql
replicate-ignore-db=sys
replicate-ignore-db=performance_schema
replicate-ignore-db=information_schema

6、重启：docker restart mysql-master mysql-slaver-01
```

```
为master授权用户来他的同步数据
1、进入master容器
docker exec -it mysql-master /bin/bash
2、mysql -uroot -p root

	1）授权root可以远程访问（主从无关，方便我们可以远程链接mysql）
	grant all privileges on *.* to 'root'@'%' IDENTIFIED BY 'root' with grant option;
	flush privileges;

	2）添加同步用户，链接master数据库，在master授权一个 复制权限的 用户
	GRANT REPLICATION SLAVE ON *.* TO 'backup'@'%' IDENTIFIED BY '123456';

3、查看master状态
show master status;
```

![1600596024636](/1600596024636.png)



```
配置slaver同步master数据
1、进入slaver容器
docker exec -it mysql-slaver-01 /bin/bash
2、mysql -uroot -p root

	1）授权root可以远程访问（主从无关，方便我们可以远程链接mysql）
	grant all privileges on *.* to 'root'@'%' IDENTIFIED BY 'root' with grant option;
	flush privileges;
	2）设置主库连接
	change master to master_host='192.168.56.10',master_user='backup',master_password='123456',master_log_file='mysql-bin.000001',master_log_pos=0,master_port=3307;
	
3、启动从库同步
start slave;

4、查看从库状态
show slave status;
```

## 2、mysql 分库分表+读写分离

### 1）原理

```
doc：https://shardingsphere.apache.org/document/current/cn/overview/

1、为什么要出现分库分表？
	答：1）缓解单库单表的压力【单表性能问题】
2、设么是分库分表？
	答：当前demo只体现了分表，放在了同一个数据库内【分库、分表逻辑可以同时出现，先根据规则分库，再分表】
	
3、使用InnoDB Cluster分库分表【不采用】
	双主模式下，两个主机都设置步长为2
	主1设置id自增，从1开始
	主2设置id自增，从2开始

4、shardingSphere有三种实现方式
	1）使用sharding-jdbc，引入sharding-jdbc【驱动】
		微服务设置分库分表策略，调用驱动保存数据时计算好 具体保存在哪个库。
	2）使用sharding-proxy，作为代理【看做数据库，不用修改源代码】
		使用中年间sharding-proxy
	3）sharding-sidecar，可以kubernetes中使用，但是还没做好

5、业务原理+demo
	例如订单表
		1、分库：不同位置的订单放在不同数据库
		2、分表：按照时间分表
```

![1600756225245](/1600756225245.png)



![1600756488828](/1600756488828.png)

###  2）实现分库分表+读写分离

```
mycat与ShardingSphere比较：
https://my.oschina.net/u/4318872/blog/4281049
```



#### # mycat

####  # ShardingSphere-proxy

以下是2主2从的示意图：

![1600756588874](/1600756588874.png)

```
doc：https://shardingsphere.apache.org/document/current/cn/overview/
配置文档：https://blog.csdn.net/qq_44826685/article/details/106190720

示意图中有多种实现方式：
	1、两个主机，两个从机。主机1数据库demo_db0，主机2demo_db1【4个容器，2个主机容器】
	2、1个主机1个从机，主机两个数据库【现在采用这个】【2个容器 】
	
实现：【参照doc】
1、下载中间件+解压
	下载mysql依赖，放到中间件解压的lib文件夹【在刚刚那个网站里面有下载地址】

=========================================================================================
2、配置认证信息+属性配置 server.yaml
authentication:
  users:
    root:
      password: root
    sharding:
      password: sharding 
      authorizedSchemas: sharding_db
props:
  executor.size: 16  # Infinite by default.
  sql.show: true

=========================================================================================
3、分库分表+读写分离 相关配置
	1）分库分表：数据分片【两个库在同一个主机上】 config-sharding.yaml
		一：分库分表是相对于写的概念，所以是主库。下面两个数据源都是配的主库【可以在不同节点，当前demo放在同一个节点下（同一容器），在同一个容器中创建了两个数据库】
		二：微服务连上sharding_db这个中间件而不是直接连接数据库
		三：主键采用雪花算法，不可以自增
        四：绑定表订单表和订单表项的关系，不需要跨库联查
  		
  		五：根据user_id分库；2、根据order_id分表
schemaName: sharding_db
#
dataSources:
  ds_0:
    url: jdbc:mysql://192.168.56.10:3307/demo_ds_0?serverTimezone=UTC&useSSL=false
    username: root
    password: root
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
  ds_1:
    url: jdbc:mysql://192.168.56.10:3307/demo_ds_1?serverTimezone=UTC&useSSL=false
    username: root
    password: root
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
    
    2）读写分离config-master_slave.yaml
        一：主从复制是InnoDB Cluster中配置【my.cnf】，读写分离是在sharding-proxy配置
		二：有几套分库分表，就配置几套读写分离
创建两个文件
############################config-master_slave.yaml############################
schemaName: sharding_db_0
#
dataSources:
  master_0_ds:
    url: jdbc:mysql://192.168.56.10:3307/demo_ds_0?serverTimezone=UTC&useSSL=false
    username: root
    password: root
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
  slave_ds_0:
    url: jdbc:mysql://192.168.56.10:3317/demo_ds_0?serverTimezone=UTC&useSSL=false
    username: root
    password: root
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
masterSlaveRule:
  name: ms_ds0
  masterDataSourceName: master_0_ds
  slaveDataSourceNames:
    - slave_ds_0
  loadBalanceAlgorithmType: ROUND_ROBIN
  
############################config-master_slave1.yaml############################
schemaName: sharding_db_1
#
dataSources:
  master_1_ds:
    url: jdbc:mysql://192.168.56.10:3307/demo_ds_1?serverTimezone=UTC&useSSL=false
    username: root
    password: root
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
  slave_ds_1:
    url: jdbc:mysql://192.168.56.10:3317/demo_ds_1?serverTimezone=UTC&useSSL=false
    username: root
    password: root
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
masterSlaveRule:
  name: ms_ds1
  masterDataSourceName: master_1_ds
  slaveDataSourceNames:
    - slave_ds_1
  loadBalanceAlgorithmType: ROUND_ROBIN

=========================================================================================
4、创建相关容器+数据库
	一：容器之前创建好了，参照集群实现
	1）docker stop mysql-master mysql-slaver-01
	2）修改配置文件
主库配置，需要同步的两个主库【分库分表的主库】
vi /mydata/mysql/master/conf/my.cnf
添加：
binlog-do-db=demo_ds_0
binlog-do-db=demo_ds_1

从库配置，需要同步的两个主库
vi /mydata/mysql/slaver/conf/my.cnf

添加：
binlog-do-db=demo_ds_0
binlog-do-db=demo_ds_1

=========================================================================================
5、docker start mysql-master mysql-slaver-01
创建数据库demo_ds_0和demo_ds_1

=========================================================================================
6、启动start.bat
	直接在文件夹目录输入cmd就可以在当前目录打开一个cmd窗口
	start.bat 3388

=========================================================================================
7、navicat新建连接，127.0.0.1:3388【这里使用navicat11，否则不会显示代理库】

=========================================================================================
8、创建测试表
在proxy代理库端创建表 t_order和t_order_item表后proxy会创建以下表
	在demo_ds_0创建t_order0、t_order1、t_order_item0、t_order_item1
	在demo_ds_1创建t_order0、t_order1、t_order_item0、t_order_item1

解释：
	主机192.168.56.10:3307，两个从机在同一容器中，但是t_order0和表 t_order1在两个主库都会创建
	主库1：demo_ds_0：表t_order0有数据，t_order1数据为空【写】
	主库2：demo_ds_1：表t_order1有数据，t_order0数据为空【写】

	从机192.168.56.10:3317，两个从机在同一容器中
	从库1：demo_ds_0：表t_order0有数据，t_order1数据为空【读】
	从库1：demo_ds_1：表t_order1有数据，t_order0数据为空【读】
```

![1600764985350](/1600764985350.png)



## 3、redis集群

### 1）集群方案【原理】

#### # 数据分区方案

##### 1.1、客户端分区

![1600767319377](/1600767319377.png)

```
客户端Jedis直接堆key作hash或求余获得指定节点存储

客户端分区方案的代表为Redis Sharding，Redis Sharding是Redis Cluster出来之前，业界普遍使用的Redis多实例集群方法。Java的Redis客户端驱动库Jedis，支持 RedisSharding功能，即ShardedJedis,以及结合缓存池的ShardedJedisPool。

优点
不使用第三方中间件，分区逻辑可控，配置简单，节点之间无关联，容易线性扩展，灵活性强。

缺点
客户端无法动态增删服务节点，客户端需要自行维护分发逻辑,客户端之间无连接共享，会造成连接浪费。
不是高可用，redis某节点宕机
```



##### 1.2、代理分区

```
与sharding proxy类似
代理分区常用方案有Twemproxy和Codis

```

![1600767541430](/1600767541430.png)



##### 1.3、哨兵机制【高可用】

```
redis-cluster未出现之前，使用哨兵机制

监控(Monitoring):哨兵(sentinel)会不断地检查你的Master和Slave是否运作正常。
提醒(Notification):当被监控的某个Redis出现问题时，哨兵(sentinel)可以通过 API向管理员或者其他应用程序发送通知。
自动故障迁移(Automatic failover)）:当主数据库出现故障时自动将从数据库转换为主数据库。【投票选举】

哨兵的原理
Redis,哨兵的三个定时任务,Redis,哨兵判定一个Redis.节点故障不可达主要就是通过三个定时监控任务来完成的:
	1、每隔1秒每个哨兵会向主节点、从节点、其他的哨兵节点发送一个“ping”命令来做心跳检测
	2、每隔2秒每个哨兵节点会向Redis节点的_sentinel_hello频道发送自己对主节点是否故障的判断以及自身的节点信息,并且其他的哨兵节点也会订阅这个频道来了解其他哨兵节点的信息以及对主节点的判断
	3、每隔10秒每个哨兵节点会向主节点和从节点发送"info replication”命令来获取最新的拓扑结构
	
    如果在定时Job1检测不到节点的心跳，会判断为"主观下线"。如果该节点还是主节点那么还会通知到其他的哨兵对该主节点进行y心跳检测，这时主观下线的票数超过了<quorum>数时，那么这个主节点确实就可能是故障不可达了，这时就由原来的主观下线变为了“客观下线"。

	故障转移和Leader选举
如果主节点被判定为客观下线之后,就要选取一个哨兵节点来完成后面的故障转移工作，选举出一个leader，这里面采用的选举算法为Raft。选举出来的哨兵leader就要来完成故障转移工作，也就是在从节点中选出一个节点来当新的主节点，这部分的具体流程可参考引用.

```

![1600768592796](/1600768592796.png)

每隔十秒：

![1600768684518](/1600768684518.png)

每隔1s：

![1600768761469](/1600768761469.png)

##### 1.4、redis-cluster【高可用+分片】

```
https://redis.io/topics/cluster-tutorial/
	Redis的官方多机部署方案，Redis Cluster。一组Redis Cluster是由多个Redis,实例组成,官方推荐我们使用6实例,其中3个为主节点,3个为从结点。一旦有主节点发生故障的时候,Redis Cluster可以选举出对应的从结点成为新的主节点继续对外服务，从而保证服务的高可用性。那么对于客户端来说，如何知道对应的key是要路由到哪一个节点呢?Redis Cluster把所有的数据划分为16384个不同的槽位，可以根据机器的性能把不同的槽位分配给不同的Redis实例，对于Redis实例来说，他们只会存储部分的Redis数据，当然，槽的数据是可以迁移的，不同的实例之间，可以通过一定的协议，进行数据迁移。
```



###### 槽+重定向

![1600832253052](/1600832253052.png)

![1600832517935](/1600832517935.png)



```
0~16383：一共16384个槽位，对key使用JAVA CRC16校验算法，然后对16383求余，算出槽位。这是一个重定向过程，如果当前节点与对应槽位没有包含关系，则重定向发送至下一节点重新判断【当前槽位是否指向自身】

redis官方方法，然后哨兵机制就可以不使用了
优点：分片+高可用都实现【相当于mysql的分库分表】

缺点+需要预防的问题：
1、key批量操作支持有限。【不再同一槽位的批量操作不再同一节点，不支持】
	类似mset、mget 操作，目前只支持对具有相同slot值的key执行批量操作。对于映射为不同slot值的key由于执行mget、mget等操作可能存在于多个节点上，因此不被支持。
	
2、key事务操作支持有限。【使用lua脚本不使用redis事务】
	只支持多key在同一节点上的事务操作，当多个key 分布在不同的节点上时无法使用事务功能。|

3、key 作为数据分区的最小粒度
4、不能将一个大的键值对象如 hash、list等映射到不同的节点。
5、不支持多数据库空间
	单机下的 Redis可以支持16个数据库（(db0~ db15)，集群模式下只能使用一个数据库空间，即db0。
6、复制结构只支持一层【只有一层主从，没有更多】
	从节点只能复制主节点，不支持嵌套树状复制结构。
7、命令大多会重定向，耗时多


```

###### 一致性hash

![1600836994345](/1600836994345.png)

```
一个闭环，计算keyhash与哪一个节点最近。

Hash倾斜
如果节点很少，容易出现倾斜，负载不均衡问题。一致性哈希算法，引入了虚拟节点，在整个环上，均衡增加若干个节点。比如al， a2，b1，b2，c1，c2，a1和a2都是属于A节点的。解决hash倾斜问题

```

#### # 高可用方案

### 2）集群实现

#### # redis-cluster

```
数据分区
```





## 3、Elasticsearch集群



## 4、RabbitMQ集群

