---
typora-copy-images-to: assert
typora-root-url: assert
---

[TOC]

```
学习网址：
k8s中文社区：https://www.kubernetes.org.cn/
k8s社区文档：http://docs.kubernetes.org.cn/

k8s官网：https://kubernetes.io/zh/
k8s官方文档：https://kubernetes.io/zh/docs/home/

kubesphere官方文档：https://kubesphere.io/zh/docs/
kubesphere文档中心：https://v2-1.docs.kubesphere.io/docs/zh-CN/release/release-v211/

```



# 三个时代

## 传统时代

 ![1651411160134](/1651411160134.png)

```json
描述：
	在硬件上搭建操作系统，部署多个APP应用
缺点：
	资源应用不充分不隔离，部署的复杂性（迁移时所有环境都需要重新搭建，可以整个系统克隆）
```

## 虚拟化时代

 ![1651411223299](/1651411223299.png)

```json
描述：
	在操作系统上搭建多个虚拟机，在每个虚拟机上部署多个APP应用
优点：
	虚拟机间资源隔离，虚拟机间互不影响，虚拟机可以做成镜像复制使用
缺点：
	资源浪费，每个虚拟机都有一个完整的系统和环境；虚拟机启动慢
```

## 容器化时代（Docker）

 ![1651589290283](/1651589290283.png)

```
描述：
	在操作系统上搭建容器运行时环境，拉取镜像创建容器，每个容器只包含运行时所需环境
优点：
	容器打包成镜像复制使用很方便，容器间运行时环境隔离，资源节约；容器启动迅速；容易实现大量的分布式部署、扩展
缺点：
	
```

![1651590070797](/1651590070797.png)

# K8S简介

```json
容器编排，在多个容器运行时环境里动态的上线容器、下线容器

docker swarm：只能编排docker运行时环境

K8S：K8S是部署和管理分布式环境的，是一个编排系统，用于构建一个强大、健壮、弹性和可扩展性的分布式系统
可以管理多种不同类型的容器运行时环境；
```

docker swarm：docker官方的编排工具，例如可以保证在docker集群中启动3个购物车容器，如果某环境宕机了，还可以动态在其他环境再启动一个

 ![1651590205032](/1651590205032.png)

## 功能

### 调度

```
决定服务在哪个节点启动
```

![1651590841425](/1651590841425.png)

### 自动恢复

```
检查节点不能响应，自动再其他节点启动
```

![1651590897207](/1651590897207.png)

### 水平伸缩

```
应用负载较高时，自动再启动一个应用
```

![1651590882952](/1651590882952.png)

## 架构

### 整体主从

```
Master+Node，每个节点包含Docker运行时环境+kubelet
```

![1651591094929](/1651591094929.png)

### 主体结构

```
UI：可视化界面
CLI：命令行，通过Master暴露的API请求K8S集群，由Master调用各Node完成响应
Master：
Node：
```

![1651591227331](/1651591227331.png)

#### Master架构

```
每个部分都可以搭建集群
1.API Server接收部署请求，存储到etcd中
2.scheduler监控etcd，选择节点启动新创建未运行的pod
```

![1651591408206](/1651591408206.png)

![1651673333646](/1651673333646.png)

---

---

---

![1651591710738](/1651591710738.png)

![1651591864853](/1651591864853.png)

##### kubectl

![1651673556099](/1651673556099.png)

示例：

![1651673576129](/1651673576129.png)

```
kubectl：命令行工具，用于发送命令行给主节点的【主节点可以接受用户界面、命令行的请求】
```

![1651672591612](/1651672591612.png)

##### Controller

```
主要是定义Pod属性
Pod副本数量
Pod类型
```

##### Deployment

```
部署，按照Controller的定义来部署维护Pod
```

##### service

```
service：一组Pod，例如多个购物车Pod，访问购物车请求访问Pod，service负载决定使用哪个pod
```

![1651672787602](/1651672787602.png)

![1651672993916](/1651672993916.png)

##### Label

```

```

![1651673120036](/1651673120036.png)

![1651673197750](/1651673197750.png)





#### Node架构

```
Node：节点组件
kubelet：代理（真正干活的），保证容器运行在pod中，CSI：每个容器的挂载目录；CNI：每个容器的网络
	创建销毁pod
	pod：一个pod包含一组容器（容器理解为docker中的容器）【多个互相依赖的容器组成一个pod单位】
kube-proxy：例如管理pod的ip，相当于网卡路由器，接收请求转发，将请求负载均衡到POD上（外界通过proxy来访问）
```

![1651591991935](/1651591991935.png)

![1651591941687](/1651591941687.png)

![1651592012149](/1651592012149.png)

## 流程叙述

```
1.通过
```



# 重要命令

**查看kubelet日志：**journalctl -u kubelet
**查看名称空间：**kubectl get ns
**查看pod（必须在指定名称空间下查看）：**kubectl get pod
**查看所有名称空间的pod：**kubectl get pods --all-namespaces
**查看所有名称空间的pod：**kubectl get pods --all-namespaces -o wide
**查看集群内所有节点：**kubectl get nodes
**master监控Node各节点：**watch kubectl get pod -n kube-system -o wide



# 一、安装kubernetes集群

```
方式一：安装minikube


方式二：k8s集群安装
```

方式一：

![1652070154659](/1652070154659.png)

方式二前置要求：

![1652103111809](/1652103111809.png)

## 1.安装基础环境

### 【部署步骤】

![1641648625504](/1641648625504.png)

### vagrant快速创建虚拟机

查看虚拟机网卡：

![1641648689796](/1641648689796.png)

```
1.拷贝k8s文件夹到一个无空格、中文的目录下，修改Vagrantfile的node.vm.box = "centos7"
	因为之前vagrant box add centos7 CentOS-7-x86_64-Vagrant-2004_01.VirtualBox.box给本地设置了一个centos7，否则使用centos/7会去仓库下载会很慢
	
2.修改每个node，使其可以远程连接
	vagrant ssh k8s-node1
	su root   密码：vagrant
	vi /etc/ssh/sshd_config	修改PasswordAuthentication yes
	service sshd restart【reboot】
	xshell连接，192.168.56.100 -- root + vagrant
	
3.NAT网络
	1）存在的问题：【默认情况下使用以下两个网卡】
		网卡一：eth0（网络地址转换NAT，所有虚拟机共用1个IP，通过映射主机端口与虚拟机端口通讯【同时映射多台虚拟机】）
		网卡二：eth1（仅主机Host-only网络，192.168.56.100）
	
	--命令ip route show：查看每个虚拟机的所有网卡和默认使用的网卡（eth0）
	--命令ip addr：查看每个虚拟机的网卡以及IP（三台虚拟机eth0网卡下的IP都是10.0.2.215）
	
	2）解决方案：
		网卡1：使用NAT网络代替  网络地址转换NAT
		网卡2：仍然使用仅主机Host-only网络，保证宿主机与k8s在同一局域网内 192.168.56.xx（虚拟机私有网络）

	3）方案实现步骤：
		1）全部关闭电源
		2）工具=》全局设定=》网络=》添加新NAT网络
		3）选中node1机器（每台虚拟机都需设置）=》网络=》网卡1=》选择NAT网络（选中刚刚创建的NAT 网络）=》高级=》刷新mac地址【每台虚拟机都会生成一个IP】
	
4.设置成功后记录每台虚拟机网卡1和网卡2的ip：
192.168.56.100=》10.0.2.4
192.168.56.101=》10.0.2.15
192.168.56.102=》10.0.2.5

5.设置linux环境
关闭防火墙:
	systemctl stop firewalld
	systemctl disable firewalld
关闭selinux:【默认安全策略 cat /etc/selinux/config】
	sed -i 's/enforcing/disabled/' /etc/selinux/config
	setenforce 0
关闭swap: 关闭内存交换【cat /etc/fstab】
	swapoff -a	只是临时关闭
	sed -ri 's/.*swap.*/#&/' /etc/fstab   选择这个
	
6.修改主机名
	1）查看主机名，如果主机名是localhost需要修改
		hostname
	2）修改
		hostnamectl set-hostname k8s-node1

7.修改每个虚拟机的hosts【集群间域名访问】
vi /etc/hosts
10.0.2.4 k8s-node1
10.0.2.15 k8s-node2
10.0.2.5 k8s-node3

8.将桥接的IPv4流量传递到iptables的链:
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system


疑难问题:
遇见提示是只读的文件系统，运行如下命令
mount -o remount rw /

date查看时间(可选)
yum install -y ntpdate
ntpdate time.windows.com同步最新时间


备份：
给三台主机做备份
```

### 安装docker

```json
1、卸载旧版本【uninstall old versions】
	sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
                  
2、安装前置依赖
sudo yum install -y yum-utils device-mapper-persistent-data lvm2

3、设置镜像地址
	方案一：
	sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
	方案二：（推荐，阿里镜像）
	sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo【这个镜像和后面提到的镜像加速不同，这里指的是下载docker本身的镜像地址】
	
4、安装docker以及docker-cli
sudo yum install -y docker-ce docker-ce-cli containerd.io

5、虚拟机开机启动：
	sudo systemctl enable docker
启动docker
	sudo systemctl start docker


6、测试
docker images -a

7、docker配置镜像下载加速
	1）默认从hub.docker下载软件镜像【很慢】
	2）修改成aliyun的镜像加速器
	3）执行【获取第一步配置的镜像加速器网址】
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://7zi5cb9i.mirror.aliyuncs.com"]
}
EOF

sudo systemctl daemon-reload
sudo systemctl restart docker
	
8.拉取
docker pull quay.io/openebs/provisioner-localpv:1.5.0
```



### 安装kubeadmin、kubelet、kubectl

![1644334589330](/1644334589330.png)

```json
1.每个节点都要安装Docker（master和Node都要，容器运行时环境）
2.每个节点安装kubeadmin（快速构建集群环境的工具，主节点设置Master，从节点join到集群）
3.每个节点安装kubelet（从节点安装kubelet【真正干活的】）
4.每个节点安装kubectl（主节点安装kubectl【命令行工具】）
```

```
添加阿里云yum源：

cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

```
查看是否安装相关源：yum list|grep kube
1、安装
yum install -y kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3

2、开机启动kubelet，要把每一个Node注册到集群中
sudo systemctl enable kubelet
sudo systemctl start kubelet【到这一步暂时还启动不起来：systemctl status kubelet，后面还有要配置的】
```

### 重置集群

```sh
当执行了以下两条命令时，需要执行底部重置集群的命令，正常安装可跳过重置集群的操作：
swapoff -a
kubeadm reset
	文档：https://blog.csdn.net/woay2008/article/details/93250137


Master执行：
systemctl daemon-reload
systemctl restart kubelet
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X  
rm -rf $HOME/.kube

Master+Node执行：
     rm -rf /var/lib/etcd
	 rm -rf /etc/cni/net.d
	 rm -rf /var/lib/kubelet
	 rm -rf /etc/kubernetes
```

## 2.部署

### 部署Master

```
1.挑选一台主机为master【当前选择10.0.2.4作为主机】
将k8s文件夹上传到master主机的root路径下（）

2.执行可执行文件拉取镜像（master_images.sh：rw- 没有执行权限【root没有执行权限】）
	1）chmod 700 master_images.sh  （rwx）
	2）./master_images.sh【https://blog.csdn.net/youzhouliu/article/details/79051516】
	3）docker images 查看镜像下载情况


3、挑选一台主机为master【当前选择10.0.2.4作为主机】
sudo kubeadm init \
--apiserver-advertise-address=10.0.2.4 \
--image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
--kubernetes-version v1.17.3 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=10.244.0.0/16

打印Your Kubernetes control-plane has initialized successfully!代表初始化成功

由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。可以手动按照我们的执行master_images.sh先拉取镜像，地址变为registry.aliyuncs.com/google_containers也可以。

科普:无类别域间路由(Classless Inter-Domain Routing、CIDR)是一个用于给用户分配IP地址以及在互联网上有效地路由IP数据包的对IP地址进行归类的方法。【1个Node有多个service，1个service有多个pod，1个pod有多个容器】
拉取可能失败，需要下载镜像。

运行完成提前复制:加入集群的令牌

pod-network-cidr：docker最小的单位是容器，pod是k8s中最小的单位（多个容器组成）
service-cidr：service是一组pod组成对外提供服务，负载均衡

4.记录上一步token
kubeadm join 10.0.2.4:6443 --token rbbfs6.or9h6i8mgdfm1czl \
    --discovery-token-ca-cert-hash sha256:9000449fd492240244794f13aab5dbd99e939dc80650b378ebee27e9c0a7f851 
【如果token过期，重新获取一个没有过期时间的token（默认2h过期）：
kubeadm token create --print-join-command --ttl=0】    

5、 生成配置文件
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

### 安装pod网络插件（CNI）

![1641658946879](/1641658946879.png)

```
部署pod网络【只有部署了网络，Node才可以加入进来】
参照doc：https://kubernetes.io/docs/concepts/cluster-administration/addons/
使用Flannel： 是一个可以用于 Kubernetes 的 overlay 网络提供者。


执行
kubectl apply -f \
https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
这个地址被墙，使用我们k8s文件夹中的yml即可，同时flannel.yml中指定的images访问不到可以去docker hub找一个

1、执行使用本地的flannel.yml【先拉取镜像】
	1）如果kube-flannel.yml中指定的images访问不到去docker.hub中查找并修改yml中的配置
		# 1、访问hub.docker.com，搜索flannel，点击from quay.io/coreos/flannel
		# 2、拷贝v0.11.0-amd64的来源jmgao1983/flannel:v0.11.0-amd64
		# 3、flannel.yml中image指定的地方，修改
	2）也可以使用quay-mirror.qiniu.com和registry.aliyuncs.com
	例如quay.io=》quay-mirror.qiniu.com  || gcr.io改为registry.aliyuncs.com

下拉镜像：quay.io/coreos/flannel:v0.10.0-s390x
如果拉取较慢，可以改为：quay-mirror.qiniu.com/coreos/flannel:v0.10.0-s390x

下拉镜像：gcr.io/google_containers/kube-proxy
可以改为： registry.aliyuncs.com/google_containers/kube-proxy

2、执行kubectl apply -f kube-flannel.yml
使用本地的yml规则安装pod组件【kubectl delete -f kube-flannel.yml：删除yml中配置的所有组件重新安装】

3、查看名称空间
kubectl get ns

4、获取所有名称空间的pod
kubectl get pods --all-namespaces -o wide
如果全部都是running就成功了，如果有不是running状态的等待一段时间，下载好镜像后就可以了

docker images|grep flannel可以查到以下镜像：
jmgao1983/flannel                                                             v0.11.0-amd64       ff281650a721        19 months ago       52.6MB

```

### 部署Node（将Node加入K8S集群）

```
1、当master是ready状态时，在k8s-node2、k8s-node3执行以下命令将其添加到master
kubeadm join 10.0.2.4:6443 --token rbbfs6.or9h6i8mgdfm1czl \
    --discovery-token-ca-cert-hash sha256:9000449fd492240244794f13aab5dbd99e939dc80650b378ebee27e9c0a7f851 

异常解决：
2.master执行watch kubectl get pod -n kube-system -o wide发现问题
文档：https://www.jianshu.com/p/bcc05427990d（CrashLoopBackOff）
	1）找到出问题的主机执行kubectl get pod获取pod的状态
	2）查看pod的详细信息：kubectl describe pod elkhost-944bcbcd4-8n9nj
	3）查看此pod日志：kubectl logs elkhost-944bcbcd4-8n9nj
	
3、master监控Node各节点
watch kubectl get pod -n kube-system -o wide
查看到node2和node3镜像下载失败
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.17.3
docker pull kubesphere/kube-proxy:v1.17.3
docker pull jmgao1983/flannel:v0.11.0-amd64
```

## 3.入门操作体验

### 部署tomcat

```
1.根据镜像创建一个部署：根据tomcat镜像（带java环境的镜像）创建一个部署（名字叫tomcat6）
kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8【1个部署对应1个pod对应1个容器】
	解析：k8s会通过主节点调用其他节点下载镜像启动容器
	

Kubectl get pods -o wide可以获取到tomcat信息【Kubectl get all】
# 容灾恢复1：在k8s-node3执行docker stop tomcat6停掉该容器，kubernetes会给node3重启一个容器
# 容灾恢复2：如果k8s-node3宕机，master会找到集群中另一个节点启动pod
```



### 暴露nginx访问



### 动态扩容测试